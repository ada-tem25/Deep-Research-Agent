import os
from datetime import datetime
from langgraph.graph import StateGraph, add_messages, START, END
from langgraph.types import Send
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage
from typing import TypedDict, Annotated, Sequence, List
from pydantic import BaseModel, Field
from langchain_google_genai import ChatGoogleGenerativeAI
import operator
from google.genai import Client
from dotenv import load_dotenv
from prompts import (query_writer_instructions, web_searcher_instructions, reflection_instructions, answer_instructions)
from utils import (get_research_topic, resolve_urls, get_citations, insert_citation_markers)



# ------------- Graph Initialization ----------------

class OverallState(TypedDict): #All the fields that the state will keep in memory between nodes, and how they are modified (add_messages / operator.add)
    messages: Annotated[list, add_messages] #conversation history
    search_query: Annotated[list, operator.add] #the search queries that will be generated by the first node
    web_research_result: Annotated[list, operator.add] #all the search results
    sources_gathered: Annotated[list, operator.add] #all the sources gathered
    initial_search_query_count: int
    max_research_loops: int #limit of research loops
    research_loop_count: int #count of research loops


builder = StateGraph(OverallState) #create a graph, with the above state as its referred state

load_dotenv() #We retrieve the API key and make sure it is well set
if os.getenv("GOOGLE_API_KEY") is None: raise ValueError("GOOGLE_API_KEY is not set")

# Used later for the Google Search API
genai_client = Client(api_key=os.getenv("GOOGLE_API_KEY"))



# --------- 1st node : Generate Search Queries -----------

class Query(BaseModel):
    query: str = Field(
        description="One of the queries necessary to the web research."
    )
    rationale: str = Field(
        description="A brief explanation of why these queries are relevant to the research topic." #It allows to audit the behaviour of the agent
    )

class QueryGenerationState(TypedDict): #the ouput node-specific state
    queries: list[Query]

class SearchQueries(BaseModel): #used to force the structure of the LLM output
    queries: list[str] = Field(
        description="A list of search queries to be used for web research."
    )
    rationale: str = Field(
        description="A brief explanation of why these queries are relevant to the research topic."
    )


def generate_query(state: OverallState) -> QueryGenerationState:
    """LangGraph node that generates search queries 
       based on the User's question. Uses Gemini 2.0 Flash.

    Args:
        state: Current graph state containing the User's question

    Returns:
        Dictionary with state update, including the list of the generated queries
    """
    print('> Generate Query')

    #We configure the model
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        temperature=0.5,
        max_retries=2,
        api_key=os.getenv("GOOGLE_API_KEY"),
    )
    llm = llm.with_structured_output(SearchQueries) #constrains the model to return an output conforming to the format of SearchQueryList

    #We adapt the prompt
    formatted_prompt = query_writer_instructions.format(
        current_date=datetime.now().strftime("%B %d, %Y"),
        research_topic=get_research_topic(state["messages"]), #This function retrieves the AI and human messages from the conversation that form the research topic
        number_queries=state["initial_search_query_count"],
    )

    result = llm.invoke(formatted_prompt) #At this point, result is of format {queries: List(str), rationale: str} (like SearchQueries)
    queries = [Query(query=q, rationale=result.rationale) for q in result.queries]
    print(f'--> {len(queries)} generated queries!')
    return {"queries": queries} #Returns a QueryGenerationState {queries: List(Query)} with Query being {query: str, rationale: str}

builder.add_edge(START, "generate_query") #this node is the first one called
builder.add_node("generate_query", generate_query)



#----------------- 1st conditional edge : parallel searches ----------

def continue_to_web_research(state: QueryGenerationState):
    """
        Routing function that sends the search queries to several instances of the web research node.
    """
    print(f'- run {len(state["queries"])} web researches in parallel')
    
    return [
        Send( #Send allows to send states in parallel to several instances of the same node
            "web_research", 
            {"search_query": search_query.query, "id": int(idx)} #WebSearchState format, as required by the next node
        )
        for idx, search_query in enumerate(state["queries"])
    ]

builder.add_conditional_edges("generate_query", continue_to_web_research, ["web_research"])



# ------------- 2nd node : Web Research ---------------

class WebSearchState(TypedDict):
    search_query: str
    id: str

def web_research(state: WebSearchState) -> OverallState:
    """LangGraph node that performs web research using the native Google 
       Search API tool.

    Args:
        state: Current graph state containing the search query and 
               research loop count
        config: Configuration for the runnable, including search API settings

    Returns:
        Dictionary with state update, including sources_gathered, 
        research_loop_count, and web_research_results
    """
    print('> Web Research :', state["search_query"])
    
    #We adapt the prompt
    formatted_prompt = web_searcher_instructions.format(
        current_date=datetime.now().strftime("%B %d, %Y"),
        research_topic=state["search_query"]
    )

    #In contrary to .invoke(), here the LLM "possesses" the Google Search tool and understands that it can use the tool to fulfill the prompt.
    response = genai_client.models.generate_content(
        model="gemini-2.0-flash",
        contents=formatted_prompt,
        config={
            "tools": [{"google_search": {}}], #A key benefit of this native integration is the grounding_metadata (urls, sources etc) returned with the response.
            "temperature": 0,
        },
    )

    #The following functions are a bit complicated : they all aim to format in the expected way the raw outputs from the LLM, with sources, urls etc. 

    #resolve the urls to short urls for saving tokens and time --> It creates a mapping between long urls and their clean short counterparts
    resolved_urls = resolve_urls(
        response.candidates[0].grounding_metadata.grounding_chunks, #This is a list of long ugly urls
        state["id"]
    )

    #gets the citations from the responses
    citations = get_citations(response, resolved_urls)
    
    #adds them to the generated recap text
    modified_text = insert_citation_markers(response.text, citations)

    #formats the citations as sources
    sources_gathered = [item for citation in citations for item in citation["segments"]]

    return { #Those are actual fields from the OverallState : the operators at the top define how they will be merged in the shared OverallState
        "search_query": [state["search_query"]], #{search_query: str, rationale: str}
        "web_research_result": [modified_text], #list of informations (str), each info containing its shortened url and website at the end
        "sources_gathered": sources_gathered, #list of {label: str (=website), short_url: str, value: str (=long url)} --> Will be used at the end to re obtain the long urls from the short ones of web_research_result
    }


#The next step will be to reflect on the web research
builder.add_node("web_research", web_research)
builder.add_edge("web_research", "reflection")



# ------------- 3rd node : Reflection ---------------

class ReflectionState(TypedDict): #node-specific state
    is_sufficient: bool
    knowledge_gap: str
    follow_up_queries: Annotated[list, operator.add]
    research_loop_count: int
    number_of_ran_queries: int

class Reflection(BaseModel): #dict to force the output format of the LLLM
    is_sufficient: bool = Field(
        description="Whether the provided summaries are sufficient to answer the user's question."
    )
    knowledge_gap: str = Field(
        description="A description of what information is missing or needs clarification."
    )
    follow_up_queries: List[str] = Field(
        description="A list of follow-up queries to address the knowledge gap."
    )

def reflection(state: OverallState) -> ReflectionState:
    """
        LangGraph node that identifies knowledge gaps (from the already retrieved info) and generates potential follow-up queries.

        Args:
            state: Current graph state containing the running summary and research topic
            config: Configuration for the runnable, including LLM provider settings

        Returns:
            Dictionary informing whether the context is sufficient or not, and of potentiel follow-up queries
    """
    print('> Reflection')

    state['research_loop_count'] = state.get("research_loop_count", 0) + 1

    #We configure the model
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash", #We use a slightly more intelligent model for the reflection
        temperature=0.5,
        max_retries=2,
        api_key=os.getenv("GOOGLE_API_KEY"),
    )
    llm = llm.with_structured_output(Reflection)

    #We adapt the prompt
    formatted_prompt = reflection_instructions.format(
        research_topic=get_research_topic(state["messages"]), #the research topic is still in the OverallState
        summaries="\n\n---\n\n".join(state["web_research_result"]), #and at this stage, all the web research results have also been put in the OverallState and can be aggregated into a summary 
    )

    result = llm.invoke(formatted_prompt)
    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "follow_up_queries": result.follow_up_queries,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
    }

builder.add_node("reflection", reflection)



#----------------- 2nd conditional edge : is context sufficient? ----------

def evaluate_research(state: ReflectionState) -> OverallState:
    
    max_research_loops = state.get('max_research_loops', 3) #state seems to also contain info of OverallState
    if (state['is_sufficient'] == True or state['research_loop_count'] > max_research_loops): 
        if (state['is_sufficient']): print('- No knowledge gap detected.')
        if (state['research_loop_count'] > max_research_loops): print(f'- Maximum amount of research loops reached : {max_research_loops}.')
        return "finalize_answer"
    elif (state['is_sufficient'] == False): 
        print(f"- Knowledge gaps detected --> {len(state['follow_up_queries'])} additional queries generated!")
        return [
            Send( #Send allows to send states in parallel to several instances of the same node
                "web_research", 
                {"search_query": query, "id": int(idx) + state["number_of_ran_queries"]} #WebSearchState format, as required by the node
            )
            for idx, query in enumerate(state["follow_up_queries"])
        ]

builder.add_conditional_edges("reflection", evaluate_research, ["web_research", "finalize_answer"])



# ----------------- 4th node : Finalizing Answer ------------------


def finalize_answer(state: OverallState) -> OverallState:
    """
        LangGraph node that finalizes the research summary.

        Prepares the final output by deduplicating and formatting sources, then
        combining them with the running summary to create a well-structured
        research report with proper citations.

        Args:
            state: Current graph state containing the running summary and sources gathered

        Returns:
            Dictionary with state update, including running_summary key containing the formatted final summary with sources
    """
    print('> Finalizing Answer!')
    formatted_prompt = answer_instructions.format(
        current_date=datetime.now().strftime("%B %d, %Y"),
        research_topic=get_research_topic(state["messages"]),
        summaries="\n---\n\n".join(state["web_research_result"]),
    )

    llm = ChatGoogleGenerativeAI(
        model='gemini-2.5-pro',
        temperature=0,
        max_retries=2,
        api_key=os.getenv("GOOGLE_API_KEY"),
    )
    result = llm.invoke(formatted_prompt)

    # Replace the short urls with the original urls and add all used urls to the sources_gathered
    unique_sources = []
    for source in state["sources_gathered"]:
        if source["short_url"] in result.content:
            result.content = result.content.replace(
                source["short_url"], source["value"]
            )
            unique_sources.append(source)

    return {
        "messages": [AIMessage(content=result.content)],
        "sources_gathered": unique_sources,
    }

builder.add_node("finalize_answer", finalize_answer)



# ----------------- Graph End ------------------

builder.add_edge("finalize_answer", END)
graph = builder.compile(name="deep-research-agent")




if __name__ == "__main__":
    graph.get_graph(xray=True).draw_mermaid_png(output_file_path='./deep_research_agent_graph.png')

    messages = [HumanMessage(content="What are the latest developments in quantum mechanics?")]
    result = graph.invoke({"messages": messages,
        "search_query": [],
        "web_research_result": [],
        "sources_gathered": [],
        "initial_search_query_count": 0,
        "max_research_loops": 2,
        "research_loop_count": 0
    })

    for m in result["messages"]:
        m.pretty_print()




















#TO DO à la fin : 
# - Tester si je peux remplacer le formattage en SearchQueries par QueryGenerationState directement, en mettant une rationale par query ? Ça allègerait la syntaxe un peu lourde du 1er node. 